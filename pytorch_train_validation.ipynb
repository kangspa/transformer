{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa319325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en\n",
    "# !python -m spacy download de\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"wmt14_translate_de-en_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc51c377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "독일어 토큰 최대 길이: 75\n",
      "영어 토큰 최대 길이: 92\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "\n",
    "# 독일어(Deutsch) 문장을 토큰화 하는 함수\n",
    "def tokenize_de(text):\n",
    "    return [token.text for token in spacy_de.tokenizer(text)]\n",
    "de_token = data['de'].apply(tokenize_de)\n",
    "\n",
    "# 영어(English) 문장을 토큰화 하는 함수\n",
    "def tokenize_en(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]\n",
    "en_token = data['en'].apply(tokenize_en)\n",
    "\n",
    "de_max_len = de_token.apply(len).max()\n",
    "en_max_len = en_token.apply(len).max()\n",
    "print(\"독일어 토큰 최대 길이:\", de_max_len)\n",
    "print(\"영어 토큰 최대 길이:\", en_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea8cf2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "독일어 토큰 사전 크기: 13941\n",
      "영어 토큰 사전 크기: 10242\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 독일어(Deutsch) 사전 제작\n",
    "de_dict = defaultdict(list)\n",
    "de_dict[\"<unk>\"] = 0\n",
    "de_dict[\"<pad>\"] = 1\n",
    "de_dict[\"<sos>\"] = 2\n",
    "de_dict[\"<eos>\"] = 3\n",
    "idx = 4\n",
    "for token_list in de_token:\n",
    "    for token in token_list:\n",
    "        if token not in de_dict:\n",
    "            de_dict[token] = idx\n",
    "            idx += 1\n",
    "print(\"독일어 토큰 사전 크기:\", len(de_dict))\n",
    "\n",
    "# 영어(English) 토큰을 숫자 인덱스로 변환\n",
    "en_dict = defaultdict(list)\n",
    "en_dict[\"<unk>\"] = 0\n",
    "en_dict[\"<pad>\"] = 1\n",
    "en_dict[\"<sos>\"] = 2\n",
    "en_dict[\"<eos>\"] = 3\n",
    "idx = 4\n",
    "for token_list in en_token:\n",
    "    for token in token_list:\n",
    "        if token not in en_dict:\n",
    "            en_dict[token] = idx\n",
    "            idx += 1\n",
    "print(\"영어 토큰 사전 크기:\", len(en_dict))\n",
    "\n",
    "dict_en = {v: k for k, v in en_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d63b6e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13941 94\n"
     ]
    }
   ],
   "source": [
    "# 최대 사전 크기 (input_dim 값 설정)\n",
    "VOCAB_SIZE = max(len(de_dict), len(en_dict))\n",
    "# Maximum sequence length\n",
    "MAX_LEN = max(de_max_len, en_max_len) + 2\n",
    "\n",
    "# 독일어(Deutsch) 토큰을 숫자 인덱스로 변환\n",
    "de_vocab = []\n",
    "for token_list in de_token:\n",
    "    tmp = [2] # 시작 토큰 추가\n",
    "    for token in token_list:\n",
    "        tmp.append(de_dict[token])\n",
    "    tmp.append(3) # 종료 토큰 추가\n",
    "    # 패딩 처리\n",
    "    if len(tmp) < MAX_LEN:\n",
    "        tmp += [1] * (MAX_LEN - len(tmp))\n",
    "    de_vocab.append(tmp)\n",
    "\n",
    "# 영어(English) 토큰을 숫자 인덱스로 변환\n",
    "en_vocab = []\n",
    "for token_list in en_token:\n",
    "    tmp = [2] # 시작 토큰 추가\n",
    "    for token in token_list:\n",
    "        tmp.append(en_dict[token])\n",
    "    tmp.append(3) # 종료 토큰 추가\n",
    "    # 패딩 처리\n",
    "    if len(tmp) < MAX_LEN:\n",
    "        tmp += [1] * (MAX_LEN - len(tmp))\n",
    "    en_vocab.append(tmp)\n",
    "\n",
    "print(VOCAB_SIZE, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f954700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 2025\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d6d98fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "input_dim = VOCAB_SIZE\n",
    "seq_len = MAX_LEN\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "dropout = 0.1\n",
    "d_ffn=2048\n",
    "\n",
    "betas = (0.9, 0.98)\n",
    "eps = 1e-9\n",
    "\n",
    "warmup_steps = 4000\n",
    "lrate = lambda step: 0 if step == 0 else (d_model ** -0.5) * min(step ** -0.5, step * warmup_steps ** -1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc25d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(de_vocab, en_vocab, test_size = 0.2, random_state = seed)\n",
    "\n",
    "x_train = torch.LongTensor(x_train)\n",
    "x_valid = torch.LongTensor(x_valid)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_valid = torch.LongTensor(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85e3043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "valid_dataset = TensorDataset(x_valid, y_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77ce1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, dropout=dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        emb_size,\n",
    "        max_len,\n",
    "        nhead,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        dim_feedforward,\n",
    "        dropout=dropout,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            d_model=emb_size, max_len=max_len, dropout=dropout\n",
    "        )\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=emb_size,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        trg,\n",
    "        src_mask,\n",
    "        tgt_mask,\n",
    "        src_padding_mask,\n",
    "        tgt_padding_mask,\n",
    "        memory_key_padding_mask,\n",
    "    ):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(\n",
    "            src=src_emb,\n",
    "            tgt=tgt_emb,\n",
    "            src_mask=src_mask,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=None,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.transformer.encoder(\n",
    "            self.positional_encoding(self.src_tok_emb(src)), src_mask\n",
    "        )\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        return self.transformer.decoder(\n",
    "            self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90fdcf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\workspace\\transformer\\env\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_tok_emb\n",
      "└ embedding\n",
      "tgt_tok_emb\n",
      "└ embedding\n",
      "positional_encoding\n",
      "└ dropout\n",
      "transformer\n",
      "└ encoder\n",
      "│  └ layers\n",
      "│  │  └ 0\n",
      "│  │  └ 1\n",
      "│  │  └ 2\n",
      "│  │  └ 3\n",
      "│  │  └ 4\n",
      "│  │  └ 5\n",
      "│  └ norm\n",
      "└ decoder\n",
      "│  └ layers\n",
      "│  │  └ 0\n",
      "│  │  └ 1\n",
      "│  │  └ 2\n",
      "│  │  └ 3\n",
      "│  │  └ 4\n",
      "│  │  └ 5\n",
      "│  └ norm\n",
      "generator\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "model = Seq2SeqTransformer(\n",
    "    num_encoder_layers=num_layers,\n",
    "    num_decoder_layers=num_layers,\n",
    "    emb_size=d_model,\n",
    "    max_len=seq_len,\n",
    "    nhead=num_heads,\n",
    "    src_vocab_size=len(de_dict),\n",
    "    tgt_vocab_size=len(en_dict),\n",
    "    dim_feedforward=d_ffn,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=betas, eps=eps)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lrate)\n",
    "\n",
    "# <pad> 토큰은 무시해야한다.\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=1, label_smoothing=0.1)\n",
    "\n",
    "for main_name, main_module in model.named_children():\n",
    "    print(main_name)\n",
    "    for sub_name, sub_module in main_module.named_children():\n",
    "        print(\"└\", sub_name)\n",
    "        for ssub_name, ssub_module in sub_module.named_children():\n",
    "            print(\"│  └\", ssub_name)\n",
    "            for sssub_name, sssub_module in ssub_module.named_children():\n",
    "                print(\"│  │  └\", sssub_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bec1d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(s):\n",
    "    mask = (torch.triu(torch.ones((s, s), device=device)) == 1).transpose(0, 1)\n",
    "    mask = (\n",
    "        mask.float()\n",
    "        .masked_fill(mask == 0, float(\"-inf\"))\n",
    "        .masked_fill(mask == 1, float(0.0))\n",
    "    )\n",
    "    return mask\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == 1).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == 1).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c71628d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\workspace\\transformer\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "[Epoch:   10] cost = 9.28551455\n",
      "[GPU] Allocated: 1215.57MB | Reserved: 5480.00MB\n",
      "[Time] 28.48 sec\n",
      "##############################\n",
      "[Epoch:   20] cost = 8.99073503\n",
      "[GPU] Allocated: 1215.57MB | Reserved: 5480.00MB\n",
      "[Time] 28.26 sec\n",
      "##############################\n",
      "[Epoch:   30] cost = 8.67281081\n",
      "[GPU] Allocated: 1215.57MB | Reserved: 5480.00MB\n",
      "[Time] 27.61 sec\n",
      "##############################\n",
      "[Epoch:   40] cost = 8.4457872\n",
      "[GPU] Allocated: 1215.57MB | Reserved: 5480.00MB\n",
      "[Time] 30.23 sec\n",
      "##############################\n",
      "[Epoch:   50] cost = 8.31056231\n",
      "[GPU] Allocated: 1215.57MB | Reserved: 5480.00MB\n",
      "[Time] 27.99 sec\n",
      "##############################\n",
      "[Epoch:   60] cost = 8.22065191\n",
      "[GPU] Allocated: 1215.57MB | Reserved: 5480.00MB\n",
      "[Time] 30.40 sec\n",
      "##############################\n",
      "[Epoch:   70] cost = 8.14255637\n",
      "[GPU] Allocated: 1215.57MB | Reserved: 5480.00MB\n",
      "[Time] 29.85 sec\n",
      "##############################\n",
      "[Epoch:   80] cost = 8.06369397\n",
      "[GPU] Allocated: 1215.57MB | Reserved: 5480.00MB\n",
      "[Time] 29.06 sec\n",
      "##############################\n",
      "[Epoch:   90] cost = 7.95807856\n",
      "[GPU] Allocated: 1215.57MB | Reserved: 5480.00MB\n",
      "[Time] 29.00 sec\n",
      "##############################\n",
      "[Epoch:  100] cost = 7.82845486\n",
      "[GPU] Allocated: 1215.57MB | Reserved: 5480.00MB\n",
      "[Time] 28.81 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "total_batch = len(data_loader)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    avg_cost = 0\n",
    "    \n",
    "    model.train()    \n",
    "    for X, Y in data_loader:\n",
    "        source_batch = X.transpose(0, 1).to(device)\n",
    "        target_batch = Y.transpose(0, 1).to(device)\n",
    "\n",
    "        target_input = target_batch[:-1, :]\n",
    "        target_output = target_batch[1:, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(\n",
    "            source_batch, target_input\n",
    "        )\n",
    "\n",
    "        hypothesis = model(\n",
    "            src=source_batch,\n",
    "            trg=target_input,\n",
    "            src_mask=src_mask,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_padding_mask=src_padding_mask,\n",
    "            tgt_padding_mask=tgt_padding_mask,\n",
    "            memory_key_padding_mask=src_padding_mask,\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost = criterion(hypothesis.reshape(-1, hypothesis.shape[-1]), target_output.reshape(-1))\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_cost += cost.item() / total_batch\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(\"#\" * 30)\n",
    "        print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))\n",
    "        allocated = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024 ** 2)\n",
    "        print(f\"[GPU] Allocated: {allocated:.2f}MB | Reserved: {reserved:.2f}MB\")\n",
    "        print(f\"[Time] {time.time() - start_time:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81ae0028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "[Valid] cost = 7.74307701\n",
      "[Valid] BLEU = 0.26\n",
      "[GPU] Allocated: 1460.12MB | Reserved: 5480.00MB\n",
      "[Time] 4599.73 sec\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# 배치 단위로 탐욕 디코딩(greedy decode)을 수행하는 함수\n",
    "def greedy_decode_batch(model, src, src_mask, src_padding_mask, max_len, start_symbol):\n",
    "    batch_size = src.size(1)\n",
    "    device = src.device\n",
    "    memory = model.encode(src, src_mask)\n",
    "    # 모든 문장의 시작을 <sos> 토큰으로 초기화\n",
    "    ys = torch.full((1, batch_size), start_symbol, dtype=torch.long, device=device)\n",
    "\n",
    "    for i in range(max_len - 1):\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(device)\n",
    "\n",
    "        # 디코더를 통해 다음 토큰 예측\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "\n",
    "        # 예측된 토큰을 결과 시퀀스에 추가\n",
    "        ys = torch.cat([ys, next_word.unsqueeze(0)], dim=0)\n",
    "\n",
    "    return ys.transpose(0, 1) # (batch_size, seq_len)으로 차원 복원\n",
    "\n",
    "\n",
    "# --- 검증 시작 ---\n",
    "model.eval()\n",
    "total_valid_cost = 0\n",
    "bleu_scores = []\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, Y in valid_loader:\n",
    "        # 텐서 차원 변환 및 디바이스로 이동\n",
    "        src = X.transpose(0, 1).to(device)\n",
    "        tgt = Y.transpose(0, 1).to(device)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        # 마스크 생성\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        # --- 1. Validation Loss 계산 ---\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        tgt_out = tgt[1:, :]\n",
    "        cost = criterion(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        total_valid_cost += cost.item()\n",
    "\n",
    "        # --- 2. BLEU 점수 계산을 위한 문장 생성 ---\n",
    "        # <sos> 토큰 인덱스는 2\n",
    "        predicted_sentences_indices = greedy_decode_batch(model, src, src_mask, src_padding_mask, max_len=MAX_LEN, start_symbol=2)\n",
    "\n",
    "        # 배치 내 모든 문장에 대해 BLEU 점수 계산\n",
    "        for i in range(X.size(0)):\n",
    "            # 정답 문장 (Y)에서 <pad>, <sos> 토큰 제외\n",
    "            target_sentence = [dict_en.get(token_id.item(), \"<unk>\") for token_id in Y[i] if token_id.item() not in [1, 2]]\n",
    "            # 예측 문장 (predicted)에서 <pad>, <sos> 토큰 제외\n",
    "            predicted_sentence = [dict_en.get(token_id.item(), \"<unk>\") for token_id in predicted_sentences_indices[i] if token_id.item() not in [1, 2]]\n",
    "\n",
    "            # <eos> 토큰이 나오면 문장 끝으로 간주\n",
    "            try:\n",
    "                eos_idx = target_sentence.index('<eos>')\n",
    "                target_sentence = target_sentence[:eos_idx]\n",
    "            except ValueError: pass\n",
    "\n",
    "            try:\n",
    "                eos_idx = predicted_sentence.index('<eos>')\n",
    "                predicted_sentence = predicted_sentence[:eos_idx]\n",
    "            except ValueError: pass\n",
    "\n",
    "            # BLEU 점수 계산\n",
    "            bleu = sentence_bleu(\n",
    "                [target_sentence], predicted_sentence,\n",
    "                weights=(0.25, 0.25, 0.25, 0.25),\n",
    "                smoothing_function=smoothie\n",
    "            )\n",
    "            bleu_scores.append(bleu)\n",
    "\n",
    "avg_valid_cost = total_valid_cost / len(valid_loader)\n",
    "avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
    "\n",
    "print(\"#\" * 30)\n",
    "print('[Valid] cost = {:>.9}'.format(avg_valid_cost))\n",
    "print('[Valid] BLEU = {:.2f}'.format(avg_bleu * 100))\n",
    "allocated = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "reserved = torch.cuda.memory_reserved() / (1024 ** 2)\n",
    "print(f\"[GPU] Allocated: {allocated:.2f}MB | Reserved: {reserved:.2f}MB\")\n",
    "print(f\"[Time] {time.time() - start_time:.2f} sec\")\n",
    "print(\"#\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
